# Лабораторная работа №1: Сегментация и аннотация текста

## Описание задания

Данная лабораторная работа реализует полный пайплайн обработки текста для задач обработки естественного языка:

1. **Сегментация текста на предложения** - разбиение текста на отдельные предложения
2. **Токенизация** - разбиение предложений на токены с обработкой сложных случаев
3. **Стемминг** - нахождение основы слова для заданного исходного слова
4. **Лемматизация** - приведение словоформы к лемме (нормальной словарной форме)
5. **Создание аннотаций** - формирование аннотированного корпуса в формате TSV

Основной задачей является корректная обработка английских текстовых данных, включая сложные случаи токенизации (телефонные номера, email адреса, адреса, эмотиконы, математические выражения, сокращения).

## Использованные технологии и инструменты

### Библиотеки и инструменты:
- **NLTK** - библиотека для обработки естественного языка
  - `sent_tokenize` - сегментация на предложения
  - `SnowballStemmer` - стемминг для английского языка
  - `WordNetLemmatizer` - лемматизация
- **pandas** - работа с датасетами
- **re** - регулярные выражения для токенизации
- **pathlib** - работа с файловой системой

### Датасет:
[Базовый новостной датасет](https://huggingface.co/datasets/wangrongsheng/ag_news)

### Модели:
- SnowballStemmer для английского языка
- WordNetLemmatizer с автоматическим определением части речи

## Результаты работы

### Реализованная функциональность:

#### 1. Токенизация с обработкой сложных случаев:
- **Телефонные номера**: +1-555-123-4567, (555) 987-6543
- **Email адреса**: test@example.com, john@company.com
- **Английские адреса**: 123 Main St, New York, NY 10001, USA
- **Эмотиконы**: :) :( :D ;) и их вариации
- **Математические выражения**: a = b*c = (c+d)^2, x = y*z = (a+b)^2
- **Сокращения**: Dr. Smith, Inc., Ltd., Mr. Johnson
- **URL**: https://example.com

#### 2. Стемминг и лемматизация:
- Использование SnowballStemmer для английского языка
- WordNetLemmatizer для лемматизации с автоматическим определением части речи
- Обработка английских текстов

#### 3. Анализ результатов:
- Статистика лемматизации
- Выявление случаев омонимии
- Анализ качества обработки
- Время затраченное на обработку тестового датасета составляет: 26.36 секунды.

### Структура проекта:
```
source/
├── main.py              # Основной скрипт
├── text_utils.py        # Общие функции обработки текста
├── stemming.py          # Модуль для стемминга
├── lemmatization.py     # Модуль для лемматизации
├── dataset_utils.py     # Утилиты для работы с датасетом
├── annotation.py        # Модуль для создания аннотаций
├── demo.py              # Демонстрация возможностей
├── analyze_results.py   # Анализ результатов лемматизации
├── requirements.txt     # Зависимости
└── dataset/
    ├── train.csv        # Обучающий датасет
    └── test.csv         # Тестовый датасет
```

### Формат выходных данных:
Результат сохраняется в формате TSV:
```
<token>    <stem>    <lemma>
```

Пример:
```
Hello	hello	hello
world	world	world
running	run	run
quickly	quick	quick
```

## Выводы

### Достигнутые результаты:
1. **Успешная реализация токенизации** - корректная обработка сложных случаев (телефонные номера, email, эмотиконы, математические выражения, сокращения)
2. **Эффективная стемминг и лемматизация** - использование специализированных библиотек NLTK
3. **Автоматизация процесса** - полный пайплайн обработки от исходного текста до аннотированного корпуса
4. **Анализ качества** - выявление случаев омонимии и статистический анализ результатов

### Выявленные проблемы и их решения:
1. **Омонимия** - некоторые слова имеют несколько возможных лемм в зависимости от контекста
2. **Сложные случаи токенизации** - требование специальных регулярных выражений для корректной обработки

### Способы решения:
1. Использование контекстной информации для разрешения омонимии
2. Разработка специализированных паттернов для сложных случаев

## Инструкция по запуску

### 1. Установка зависимостей:
```bash
pip install -r requirements.txt
```

### 2. Основной скрипт:
```bash
python main.py dataset/train.csv
python main.py dataset/train.csv assets/annotated-corpus
```

### 3. Демонстрация возможностей:
```bash
python demo.py
python demo.py dataset/train.csv
```

### 4. Создание аннотаций:
```bash
python annotation.py dataset/train.csv
python annotation.py dataset/train.csv assets/annotated-corpus
```

### 5. Анализ результатов:
```bash
python analyze_results.py assets/annotated-corpus/test
```